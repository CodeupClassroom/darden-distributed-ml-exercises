{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark / Distributed ML Overview\n",
    "\n",
    "How do we work with big data? Apache Spark\n",
    "\n",
    "- Lessons: overview, env setup, spark api, wrangle, explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When or why would I use spark?\n",
    "\n",
    "- Dealing with big data; 4 Vs velocity, volume, variety, veracity\n",
    "- When the data can't live on your laptop\n",
    "- Spark streaming\n",
    "- Cloud analysis of data that lives in the cloud\n",
    "- Alternatives: hadoop + dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "\n",
    "- scala on the JVM\n",
    "- client libraries (pyspark) that talk to a running spark instance\n",
    "    - regardless of the specific library, all the spark code that's run is the same\n",
    "    - cluster manager can optimize queries\n",
    "- **Driver**: what kickstarts everythign\n",
    "- **Cluster Manager**: a cloud computer that orchestrates all of spark\n",
    "- **Executor**: 1+ computers that do the work\n",
    "- Databricks: notebook environment for managing spark\n",
    "- **Local Mode**: everything on one laptop\n",
    "    - use case: data larger than memory, but smaller than storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Work\n",
    "\n",
    "- parallel work has a higher overhead, but better performance at scale\n",
    "- two levels: executors (1+) and partitions (1+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Dataframes\n",
    "\n",
    "- abstracts all of the above\n",
    "- lazy -- don't do work until they have to\n",
    "- **transformations** and **actions**\n",
    "- efficiency + optimization\n",
    "    - example query using telco_churn (8000 rows)\n",
    "        1. Convert tenure to years (tenure / 12) (8000)\n",
    "        1. find all the customers w/ greater than 2 years tenure (4000)\n",
    "        1. find all the fiber optic customers (2000)\n",
    "        1. average monthly charges by payment_type\n",
    "        1. show me the results\n",
    "    - spark might re-arrange to be more efficient, to do less work\n",
    "        1. find all the fiber optic customers (2000)\n",
    "        1. convert tenure to years (tenure / 12)\n",
    "        1. gt 2 years tenure\n",
    "        1. average by payment_type\n",
    "        1. results\n",
    "- shuffle -- when an operation depends on data in different partitions\n",
    "    - filter (e.g. just fiber optic customers) -- no shuffle\n",
    "    - sort by total charges descending -- shuffle\n",
    "    - biggest the performance concern in spark optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
